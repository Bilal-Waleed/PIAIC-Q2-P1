{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bilal-Waleed/PIAIC-Q2-P1/blob/main/PIAIC_Q2_Project_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIsJLr2Xwjaj"
      },
      "source": [
        "# **Project 1: LangChain Hello World Project**\n",
        "\n",
        "In this Project, we will create a simple LangChain Colab Notebook that uses the Google Gemini Flash 2.0 model to answer user questions.\n",
        "\n",
        "The project Github repo is: https://github.com/panaversity/learn-agentic-ai/tree/main/02_generative_ai_for_beginners/PROJECTS/01_hello_gemini\n",
        "\n",
        "# **Next Steps:**\n",
        "\n",
        "**Experiment with Prompts:** Add more prompt templates to see how the model responds.\n",
        "\n",
        "**Add Memory:** Use LangChainâ€™s memory feature to make the interaction multi-turn.\n",
        "\n",
        "**Integrate Tools:** Extend the chain to include tools like database queries or APIs.\n",
        "\n",
        "**Explore Gemini Features:** Adjust temperature, max tokens, and other parameters to optimize responses.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "busFFyzZ7hmF"
      },
      "outputs": [],
      "source": [
        "!pip install langchain\n",
        "!pip install langchain-google-genai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4FNpDVTqSdY"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from google.colab import userdata\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "gemini_api_key = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash-exp\",\n",
        "    max_retries=3,\n",
        "    temperature=0.2,\n",
        "    max_output_tokens=200,\n",
        "    api_key=gemini_api_key\n",
        ")\n",
        "\n",
        "memory = ConversationBufferWindowMemory(k=5, memory_key=\"chat_history\", return_messages=True)\n",
        "\n",
        "informative_prompt = PromptTemplate(\n",
        "    input_variables=[\"chat_history\", \"question\"],\n",
        "    template=(\n",
        "        \"The following is a conversation between you and a user. \"\n",
        "        \"You are a helpful assistant. Always provide a detailed and factual answer. \"\n",
        "        \"If the question is new, answer it from your knowledge base without depending on the conversation history.\\n\\n\"\n",
        "        \"Conversation History:\\n{chat_history}\\n\\n\"\n",
        "        \"User's Current Question:\\n{question}\"\n",
        "    )\n",
        ")\n",
        "\n",
        "concise_prompt = PromptTemplate(\n",
        "    input_variables=[\"chat_history\", \"question\"],\n",
        "    template=(\n",
        "        \"The following is a conversation between you and a user. \"\n",
        "        \"You are a helpful assistant. Provide a concise answer to the question. \"\n",
        "        \"If the question is new, answer it from your knowledge base without depending on the conversation history.\\n\\n\"\n",
        "        \"Conversation History:\\n{chat_history}\\n\\n\"\n",
        "        \"User's Current Question:\\n{question}\"\n",
        "    )\n",
        ")\n",
        "\n",
        "chain_informative = LLMChain(llm=llm, prompt=informative_prompt, memory=memory)\n",
        "chain_concise = LLMChain(llm=llm, prompt=concise_prompt, memory=memory)\n",
        "\n",
        "print(\"Welcome! You can ask questions.\")\n",
        "print(\"Type 'exit' to end the conversation.\\n\")\n",
        "\n",
        "while True:\n",
        "    user_question = input(\"Your question: \")\n",
        "\n",
        "    if user_question.lower() == \"exit\":\n",
        "        print(\"\\nGoodbye! Have a great day!\")\n",
        "        break\n",
        "\n",
        "    detailed_response = chain_informative.run({\"question\": user_question})\n",
        "    print(f\"\\nDetailed Response: {detailed_response}\\n\")\n",
        "\n",
        "    concise_choice = input(\"Do you want a concise version of the answer? Type 'yes' for concise or press Enter to skip, or 'exit' to quit: \").lower()\n",
        "\n",
        "    if concise_choice == \"exit\":\n",
        "        print(\"\\nGoodbye! Have a great day!\")\n",
        "        break\n",
        "\n",
        "    if concise_choice == \"yes\":\n",
        "        concise_response = chain_concise.run({\"question\": user_question})\n",
        "        print(f\"\\nConcise Response: {concise_response}\\n\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNjH/CveHOK7fq/ABNoqOkr",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}